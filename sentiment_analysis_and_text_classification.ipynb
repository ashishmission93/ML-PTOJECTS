{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXgD9VKUGqQtiAwF1/zjgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishmission93/ML-PTOJECTS/blob/main/sentiment_analysis_and_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example code demonstrating how to perform sentiment analysis and text classification on movie reviews dataset:"
      ],
      "metadata": {
        "id": "OQVcyKpboZ3E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxBt1Knknpd1",
        "outputId": "0865948e-e84a-41b1-ff5b-b4264e9b6333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Accuracy: 0.8175\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load movie reviews dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents\n",
        "import random\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Define feature extractor and label set\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "# Define feature extractor function\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# Extract features from documents\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classifier (e.g., Support Vector Machine)\n",
        "classifier = nltk.classify.SklearnClassifier(SVC(kernel='linear'))\n",
        "classifier.train(train_set)\n",
        "\n",
        "# Test the classifier\n",
        "y_true = [category for (features, category) in test_set]\n",
        "y_pred = [classifier.classify(features) for (features, category) in test_set]\n",
        "\n",
        "# Evaluate classifier accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Classifier Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code snippet demonstrates:\n",
        "\n",
        "Downloading and loading the movie reviews dataset from NLTK.\n",
        "Preprocessing the text data by tokenizing words and extracting relevant features.\n",
        "Splitting the dataset into training and testing sets.\n",
        "Training a Support Vector Machine (SVM) classifier using the training set.\n",
        "Testing the classifier on the testing set and evaluating its accuracy."
      ],
      "metadata": {
        "id": "7mvPMhNxo76_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load movie reviews dataset\n",
        "documents = [(movie_reviews.raw(fileid), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents\n",
        "import random\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Preprocess the documents\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase conversion\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to documents\n",
        "documents = [(preprocess_text(text), category) for text, category in documents]\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X = [text for text, _ in documents]\n",
        "y = [category for _, category in documents]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a classifier (e.g., Support Vector Machine)\n",
        "classifier = SVC(kernel='linear')\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Test the classifier\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate classifier accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Classifier Accuracy:\", accuracy)\n",
        "\n",
        "# Perform Topic Modeling using Latent Dirichlet Allocation (LDA)\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "X_lda = lda.fit_transform(X_train_tfidf)\n",
        "\n",
        "# Display top words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_idx = topic.argsort()[:-10 - 1:-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"Topic {topic_idx+1}: {', '.join(top_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU9QqxRspTID",
        "outputId": "e509c0ee-dab2-4f4e-8524-34358450be21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Accuracy: 0.84\n",
            "Topic 1: mulan, disney, army, animated, serve, murphy, chinese, eddie, animation, spectacular\n",
            "Topic 2: mulan, wrestling, larry, flynt, spawn, shrek, vampires, truman, carpenter, spice\n",
            "Topic 3: truman, flynt, wrestling, spawn, larry, carrey, norton, speech, freedom, court\n",
            "Topic 4: nbsp, television, carter, meaning, fbi, appearance, series, culture, truth, independence\n",
            "Topic 5: film, movie, one, like, even, good, story, time, would, characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this expanded code:\n",
        "\n",
        "We preprocess the text data by tokenizing, converting to lowercase, removing non-alphabetic tokens, and filtering out stopwords.\n",
        "We split the dataset into training and testing sets.\n",
        "We vectorize the text data using TF-IDF (Term Frequency-Inverse Document Frequency) representation.\n",
        "We train a Support Vector Machine (SVM) classifier on the TF-IDF vectors.\n",
        "We evaluate the classifier's accuracy on the testing set.\n",
        "We perform topic modeling using Latent Dirichlet Allocation (LDA) to identify topics in the documents.\n",
        "We display the top words for each topic discovered by LDA."
      ],
      "metadata": {
        "id": "v9hh3ZfwpZ3q"
      }
    }
  ]
}