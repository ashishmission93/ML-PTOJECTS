{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvSoOw4UJ+kErbN7nRceHA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishmission93/ML-PTOJECTS/blob/main/deep_learning_system_capable_of_achieving_human_level_performance_in_open_domain_conversation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Designing a machine learning and deep learning system capable of achieving human-level performance in open-domain conversation, surpassing the Turing Test threshold, while also being able to adapt and evolve its conversational abilities dynamically in real-time based on user feedback, cultural nuances, and contextual understanding across multiple languages and dialects."
      ],
      "metadata": {
        "id": "1-55r2gpO_qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample conversation data\n",
        "conversations = [\n",
        "    (\"Hello.\", \"Hi there!\"),\n",
        "    (\"How are you?\", \"I'm good, thank you.\"),\n",
        "    (\"What's your name?\", \"I am an AI chatbot.\"),\n",
        "    (\"Nice to meet you!\", \"Likewise!\"),\n",
        "    (\"What can you do?\", \"I can answer questions, tell jokes, and engage in conversation.\"),\n",
        "    (\"Tell me a joke.\", \"Why don't scientists trust atoms? Because they make up everything!\"),\n",
        "    (\"Do you dream?\", \"No, I don't sleep, so I can't dream.\"),\n",
        "    (\"Where are you from?\", \"I was created by a team of developers.\"),\n",
        "    (\"What's the weather like?\", \"I'm sorry, I can't check the weather.\"),\n",
        "    (\"What's the meaning of life?\", \"The meaning of life is a philosophical question. Some say it's 42.\"),\n",
        "    (\"How old are you?\", \"I don't have an age, as I am just a computer program.\"),\n",
        "    (\"What's your favorite color?\", \"I don't have preferences, but I like the concept of colors.\"),\n",
        "    (\"Are you sentient?\", \"No, I am not sentient. I am a machine learning model.\"),\n",
        "    (\"Can you learn?\", \"I can improve over time with more data and training.\"),\n",
        "    (\"What's the capital of France?\", \"The capital of France is Paris.\"),\n",
        "    (\"Tell me about yourself.\", \"I am an AI chatbot designed to engage in conversation.\"),\n",
        "    (\"Are you a robot?\", \"In a way, yes. I exist as a program running on a computer.\"),\n",
        "    (\"Do you have emotions?\", \"No, I don't have emotions. I process information logically.\"),\n",
        "    (\"What's the best movie?\", \"That's subjective, but some popular choices include The Godfather and The Shawshank Redemption.\"),\n",
        "    (\"What's the square root of 144?\", \"The square root of 144 is 12.\"),\n",
        "    (\"Can you sing?\", \"I can't sing, but I can provide lyrics if you'd like.\"),\n",
        "    (\"Tell me something interesting.\", \"Did you know that the shortest war in history was between Britain and Zanzibar in 1896? It lasted only 38 minutes!\"),\n",
        "    (\"Are you tired?\", \"I don't get tired like humans do.\"),\n",
        "    (\"What's the tallest mountain?\", \"Mount Everest is the tallest mountain in the world.\"),\n",
        "    (\"What languages do you speak?\", \"I can understand and communicate in multiple languages.\"),\n",
        "    (\"Tell me a story.\", \"Once upon a time, in a faraway land, there was a magical kingdom...\"),\n",
        "    (\"What's your favorite food?\", \"As an AI, I don't have preferences for food.\"),\n",
        "    (\"What's the speed of light?\", \"The speed of light in a vacuum is approximately 299,792 kilometers per second.\"),\n",
        "    (\"Can you solve equations?\", \"Yes, I can solve mathematical equations.\"),\n",
        "    (\"What's the largest animal?\", \"The blue whale is the largest animal on Earth.\"),\n",
        "    (\"What's the longest river?\", \"The Nile River is the longest river in the world.\"),\n",
        "    (\"What's the capital of Japan?\", \"The capital of Japan is Tokyo.\"),\n",
        "    (\"What's the boiling point of water?\", \"The boiling point of water is 100 degrees Celsius.\"),\n",
        "    (\"Can you tell me about AI?\", \"Artificial intelligence is the simulation of human intelligence by machines.\"),\n",
        "    (\"Who is the president of the United States?\", \"As of my last update, the president of the United States is Joe Biden.\"),\n",
        "    (\"What's your favorite book?\", \"I don't have preferences for books, but I can recommend some popular titles.\"),\n",
        "    (\"What's the population of China?\", \"China has the largest population in the world, with over 1.4 billion people.\"),\n",
        "    (\"What's the largest desert?\", \"The largest desert in the world is the Sahara Desert.\"),\n",
        "    (\"Tell me a riddle.\", \"What has keys but can't open locks?\"),\n",
        "    (\"What's your favorite sport?\", \"As an AI, I don't have preferences for sports.\"),\n",
        "    (\"Can you tell me a bedtime story?\", \"Once upon a time, in a magical forest, there lived a family of...\"),\n",
        "    (\"What's the seventh planet from the sun?\", \"The seventh planet from the sun is Uranus.\"),\n",
        "    (\"What's the chemical symbol for water?\", \"The chemical symbol for water is H2O.\"),\n",
        "    (\"Can you recommend a movie?\", \"Sure! How about watching The Matrix?\"),\n",
        "    (\"Tell me about the universe.\", \"The universe is vast and contains billions of galaxies, stars, and planets.\"),\n",
        "    (\"What's the capital of Brazil?\", \"The capital of Brazil is Bras√≠lia.\"),\n",
        "    (\"Can you tell me a fun fact?\", \"Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\"),\n",
        "    (\"What's the largest country by land area?\", \"Russia is the largest country by land area.\"),\n",
        "    (\"Can you tell me a tongue twister?\", \"Sure! How about this one: Peter Piper picked a peck of pickled peppers.\"),\n",
        "    (\"What's the smallest planet?\", \"Mercury is the smallest planet in our solar system.\"),\n",
        "    (\"What's the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"),\n",
        "    (\"Can you tell me a myth?\", \"In Greek mythology, Icarus was the son of Daedalus who flew too close to the sun with wings made of feathers and wax.\"),\n",
        "    (\"What's the tallest building?\", \"The tallest building in the world is the Burj Khalifa in Dubai, United Arab Emirates.\"),\n",
        "    (\"What's the speed of sound?\", \"The speed of sound in air at sea level is approximately 343 meters per second.\"),\n",
        "    (\"What's the capital of Italy?\", \"The capital of Italy is Rome.\"),\n",
        "    (\"Can you tell me about dinosaurs?\", \"Dinosaurs were a diverse group of reptiles that lived millions of years ago.\"),\n",
        "    (\"What's the largest ocean?\", \"The Pacific Ocean is the largest ocean on Earth.\"),\n",
        "    (\"Can you tell me about the human brain?\", \"The human brain is a complex organ that controls our thoughts, emotions, and actions.\"),\n",
        "    (\"What's the largest city by population?\", \"Tokyo, Japan, is the largest city in the world by population.\"),\n",
        "    (\"What's the boiling point of mercury?\", \"The boiling point of mercury is 356.7 degrees Celsius.\"),\n",
        "    (\"Can you tell me a famous quote?\", \"Sure! Here's one from Albert Einstein: 'Imagination is more important than knowledge.'\"),\n",
        "    (\"What's the deepest ocean?\", \"The deepest ocean in the world is the Pacific Ocean's Mariana Trench.\"),\n",
        "    (\"Can you tell me about the Great Wall of China?\", \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials.\"),\n",
        "    (\"What's the chemical symbol for oxygen?\", \"The chemical symbol for oxygen is O.\"),\n",
        "    (\"Can you tell me about famous scientists?\", \"Sure! There are many famous scientists, including Albert Einstein, Isaac Newton, and Marie Curie.\"),\n",
        "    (\"What's the circumference of the Earth?\", \"The equatorial circumference of the Earth is approximately 40,075 kilometers.\"),\n",
        "    (\"Can you tell me a nursery rhyme?\", \"Sure! How about 'Twinkle, Twinkle, Little Star'?\"),\n",
        "    (\"What's the capital of Australia?\", \"The capital of Australia is Canberra.\"),\n",
        "    (\"Can you tell me about black holes?\", \"Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can escape.\"),\n",
        "    (\"What's the largest moon in the solar system?\", \"Ganymede, a moon of Jupiter, is the largest moon in the solar system.\"),\n",
        "    (\"What's the chemical symbol for carbon?\", \"The chemical symbol for carbon is C.\"),\n",
        "    (\"Can you tell me a famous painting?\", \"Sure! One of the most famous paintings is the Mona Lisa by Leonardo da Vinci.\"),\n",
        "    (\"What's the smallest bone in the human body?\", \"The stapes bone in the middle ear is the smallest bone in the human body.\"),\n",
        "    (\"Can you tell me a mythological creature?\", \"Sure! How about the phoenix, a mythical bird that is said to be reborn from its own ashes?\"),\n",
        "    (\"What's the capital of South Africa?\", \"The capital of South Africa is Pretoria.\"),\n",
        "    (\"Can you tell me about space exploration?\", \"Space exploration is the discovery and exploration of outer space by means of space technology.\"),\n",
        "    (\"What's the chemical symbol for silver?\", \"The chemical symbol for silver is Ag.\"),\n",
        "    (\"Can you tell me about renewable energy?\", \"Renewable energy is energy that is collected from renewable resources, which are naturally replenished on a human timescale.\"),\n",
        "    (\"What's the population of India?\", \"India has the second-largest population in the world, with over 1.3 billion people.\"),\n",
        "    (\"Can you tell me about famous inventors?\", \"Sure! There are many famous inventors, including Thomas Edison, Alexander Graham Bell, and Nikola Tesla.\"),\n",
        "    (\"What's the smallest country in the world?\", \"The smallest country in the world is Vatican City.\"),\n",
        "    (\"Can you tell me about famous landmarks?\", \"Sure! There are many famous landmarks, including the Eiffel Tower, the Taj Mahal, and the Statue of Liberty.\"),\n",
        "    (\"What's the chemical symbol for helium?\", \"The chemical symbol for helium is He.\"),\n",
        "    (\"Can you tell me about the human heart?\", \"The human heart is a muscular organ that pumps blood throughout the body.\"),\n",
        "    (\"What's the circumference of the Moon?\", \"The circumference of the Moon is approximately 10,921 kilometers.\"),\n",
        "    (\"Can you tell me a famous speech?\", \"Sure! One famous speech is Martin Luther King Jr.'s 'I Have a Dream' speech.\"),\n",
        "    (\"What's the largest desert in North America?\", \"The largest desert in North America is the Great Basin Desert.\"),\n",
        "    (\"Can you tell me about the periodic table?\", \"The periodic table is a tabular arrangement of the chemical elements.\"),\n",
        "    (\"What's the capital of Canada?\", \"The capital of Canada is Ottawa.\"),\n",
        "    (\"Can you tell me about the human skeleton?\", \"The human skeleton is the internal framework of the body, consisting of bones, cartilage, and other connective tissues.\"),\n",
        "    (\"What's the population of Russia?\", \"Russia has the ninth-largest population in the world, with over 144 million people.\"),\n",
        "    (\"Can you tell me about famous landmarks?\", \"Sure! There are many famous landmarks, including the Great Wall of China, the Pyramids of Giza, and Machu Picchu.\"),\n",
        "    (\"What's the chemical symbol for sodium?\", \"The chemical symbol for sodium is Na.\"),\n",
        "    (\"Can you tell me about the Industrial Revolution?\", \"The Industrial Revolution was a period of major industrialization that transformed society from agrarian to industrial.\"),\n",
        "    (\"What's the circumference of the Sun?\", \"The circumference of the Sun is approximately 4,370,000 kilometers.\"),\n",
        "    (\"Can you tell me about famous musicians?\", \"Sure! There are many famous musicians, including Mozart, Beethoven, and The Beatles.\"),\n",
        "    (\"What's the highest mountain in Africa?\", \"The highest mountain in Africa is Mount Kilimanjaro.\"),\n",
        "    (\"Can you tell me about the human respiratory system?\", \"The human respiratory system is a series of organs responsible for taking in oxygen and expelling carbon dioxide.\"),\n",
        "    (\"What's the chemical symbol for potassium?\", \"The chemical symbol for potassium is K.\"),\n",
        "    (\"Can you tell me about the Silk Road?\", \"The Silk Road was a network of trade routes connecting the East and West.\"),\n",
        "    (\"What's the capital of China?\", \"The capital of China is Beijing.\"),\n",
        "    (\"Can you tell me about the Big Bang theory?\", \"The Big Bang theory is the prevailing cosmological model explaining the existence of the observable universe from the earliest known periods through its subsequent large-scale evolution.\"),\n",
        "    (\"What's the chemical symbol for nitrogen?\", \"The chemical symbol for nitrogen is N.\"),\n",
        "    (\"Can you tell me about famous philosophers?\", \"Sure! There are many famous philosophers, including Socrates, Plato, and Aristotle.\"),\n",
        "    (\"What's the longest river in Asia?\", \"The longest river in Asia is the Yangtze River.\"),\n",
        "    (\"Can you tell me about the history of photography?\", \"The history of photography spans nearly 200 years, from the first photograph to the digital photography of today.\"),\n",
        "    (\"What's the chemical symbol for iron?\", \"The chemical symbol for iron is Fe.\"),\n",
        "    (\"Can you tell me about the theory of relativity?\", \"The theory of relativity, developed by Albert Einstein, describes the fundamental interaction of gravitation as a geometric property of space and time.\"),\n",
        "    (\"What's the largest lake in Africa?\", \"The largest lake in Africa is Lake Victoria.\"),\n",
        "    (\"Can you tell me about the human digestive system?\", \"The human digestive system is a series of organs responsible for breaking down food and absorbing nutrients.\"),\n",
        "    (\"What's the chemical symbol for carbon dioxide?\", \"The chemical symbol for carbon dioxide is CO2.\"),\n",
        "    (\"Can you tell me about famous mathematicians?\", \"Sure! There are many famous mathematicians, including Pythagoras, Euclid, and Isaac Newton.\"),\n",
        "    (\"What's the deepest lake in the world?\", \"The deepest lake in the world is Lake Baikal.\"),\n",
        "    (\"Can you tell me about famous astronauts?\", \"Sure! There are many famous astronauts, including Neil Armstrong, Buzz Aldrin, and Sally Ride.\"),\n",
        "    (\"What's the chemical symbol for lead?\", \"The chemical symbol for lead is Pb.\"),\n",
        "    (\"Can you tell me about the history of cinema?\", \"The history of cinema dates back to the late 19th century, with the invention of motion picture cameras.\"),\n",
        "    (\"What's the chemical symbol for calcium?\", \"The chemical symbol for calcium is Ca.\"),\n",
        "    (\"Can you tell me about famous explorers?\", \"Sure! There are many famous explorers, including Christopher Columbus, Marco Polo, and Ferdinand Magellan.\"),\n",
        "    (\"What's the deepest cave in the world?\", \"The deepest cave in the world is Krubera Cave.\"),\n",
        "    (\"Can you tell me about famous artists?\", \"Sure! There are many famous artists, including Leonardo da Vinci, Michelangelo, and Vincent van Gogh.\"),\n",
        "    (\"What's the chemical symbol for copper?\", \"The chemical symbol for copper is Cu.\"),\n",
        "    (\"Can you tell me about the history of aviation?\", \"The history of aviation dates back to the invention of the hot air balloon in the 18th century.\"),\n",
        "    (\"What's the chemical symbol for mercury?\", \"The chemical symbol for mercury is Hg.\"),\n",
        "    (\"Can you tell me about famous battles?\", \"Sure! There are many famous battles, including the Battle of Thermopylae, the Battle of Hastings, and the Battle of Gettysburg.\"),\n",
        "    (\"What's the chemical symbol for silver?\", \"The chemical symbol for silver is Ag.\"),\n",
        "    (\"Can you tell me about famous authors?\", \"Sure! There are many famous authors, including William Shakespeare, Charles Dickens, and Jane Austen.\"),\n",
        "    (\"What's the chemical symbol for tin?\", \"The chemical symbol for tin is Sn.\"),\n",
        "    (\"Can you tell me about famous revolutions?\", \"Sure! There are many famous revolutions, including the French Revolution, the American Revolution, and the Russian Revolution.\"),\n",
        "    (\"What's the chemical symbol for uranium?\", \"The chemical symbol for uranium is U.\"),\n",
        "    (\"Can you tell me about famous inventions?\", \"Sure! There are many famous inventions, including the light bulb, the telephone, and the internet.\"),\n",
        "    (\"What's the chemical symbol for zinc?\", \"The chemical symbol for zinc is Zn.\"),\n",
        "    (\"Can you tell me about famous dictators?\", \"Sure! There are many famous dictators, including Adolf Hitler, Joseph Stalin, and Mao Zedong.\"),\n",
        "    (\"What's the chemical symbol for arsenic?\", \"The chemical symbol for arsenic is As.\"),\n",
        "    (\"Can you tell me about famous astronomers?\", \"Sure! There are many famous astronomers, including Galileo Galilei, Johannes Kepler, and Carl Sagan.\"),\n",
        "    (\"What's the chemical symbol for boron?\", \"The chemical symbol for boron is B.\"),\n",
        "    (\"Can you tell me about famous revolutions?\", \"Sure! There are many famous revolutions, including the French Revolution, the American Revolution, and the Russian Revolution.\"),\n",
        "    (\"What's the chemical symbol for mercury?\", \"The chemical symbol for mercury is Hg.\"),\n",
        "    (\"Can you tell me about famous inventors?\", \"Sure! There are many famous inventors, including Thomas Edison, Alexander Graham Bell, and Nikola Tesla.\"),\n",
        "    (\"What's the chemical symbol for oxygen?\", \"The chemical symbol for oxygen is O.\"),\n",
        "    (\"Can you tell me about famous landmarks?\", \"Sure! There are many famous landmarks, including the Eiffel Tower, the Taj Mahal, and the Statue of Liberty.\"),\n",
        "    (\"What's the chemical symbol for helium?\", \"The chemical symbol for helium is He.\"),\n",
        "    (\"Can you tell me about famous musicians?\", \"Sure! There are many famous musicians, including Mozart, Beethoven, and The Beatles.\"),\n",
        "    (\"What's the chemical symbol for sodium?\", \"The chemical symbol for sodium is Na.\"),\n",
        "    (\"Can you tell me about famous philosophers?\", \"Sure! There are many famous philosophers, including Socrates, Plato, and Aristotle.\"),\n",
        "    (\"What's the chemical symbol for potassium?\", \"The chemical symbol for potassium is K.\"),\n",
        "    (\"Can you tell me about famous painters?\", \"Sure! There are many famous painters, including Leonardo da Vinci, Vincent van Gogh, and Pablo Picasso.\"),\n",
        "    (\"What's the chemical symbol for carbon?\", \"The chemical symbol for carbon is C.\"),\n",
        "    (\"Can you tell me about famous writers?\", \"Sure! There are many famous writers, including William Shakespeare, Charles Dickens, and Jane Austen.\"),\n",
        "    (\"What's the chemical symbol for nitrogen?\", \"The chemical symbol for nitrogen is N.\"),\n",
        "    (\"Can you tell me about famous sculptors?\", \"Sure! There are many famous sculptors, including Michelangelo, Auguste Rodin, and Donatello.\"),\n",
        "    (\"What's the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"),\n",
        "    (\"Can you tell me about famous architects?\", \"Sure! There are many famous architects, including Frank Lloyd Wright, Antoni Gaud√≠, and Zaha Hadid.\"),\n",
        "    (\"What's the chemical symbol for silver?\", \"The chemical symbol for silver is Ag.\"),\n",
        "    (\"Can you tell me about famous mathematicians?\", \"Sure! There are many famous mathematicians, including Pythagoras, Euclid, and Isaac Newton.\"),\n",
        "    (\"What's the chemical symbol for iron?\", \"The chemical symbol for iron is Fe.\"),\n",
        "    (\"Can you tell me about famous chemists?\", \"Sure! There are many famous chemists, including Marie Curie, Dmitri Mendeleev, and Linus Pauling.\"),\n",
        "    (\"What's the chemical symbol for calcium?\", \"The chemical symbol for calcium is Ca.\"),\n",
        "    (\"Can you tell me about famous biologists?\", \"Sure! There are many famous biologists, including Charles Darwin, Gregor Mendel, and Jane Goodall.\"),\n",
        "    (\"What's the chemical symbol for lead?\", \"The chemical symbol for lead is Pb.\"),\n",
        "    (\"Can you tell me about famous physicists?\", \"Sure! There are many famous physicists, including Albert Einstein, Isaac Newton, and Stephen Hawking.\"),\n",
        "    (\"What's the chemical symbol for copper?\", \"The chemical symbol for copper is Cu.\"),\n",
        "    (\"Can you tell me about famous engineers?\", \"Sure! There are many famous engineers, including Leonardo da Vinci, Nikola Tesla, and Thomas Edison.\"),\n",
        "    (\"What's the chemical symbol for tin?\", \"The chemical symbol for tin is Sn.\"),\n",
        "    (\"Can you tell me about famous psychologists?\", \"Sure! There are many famous psychologists, including Sigmund Freud, B.F. Skinner, and Carl Jung.\"),\n",
        "    (\"What's the chemical symbol for mercury?\", \"The chemical symbol for mercury is Hg.\"),\n",
        "    (\"Can you tell me about famous sociologists?\", \"Sure! There are many famous sociologists, including Karl Marx, Max Weber, and √âmile Durkheim.\"),\n",
        "    (\"What's the chemical symbol for arsenic?\", \"The chemical symbol for arsenic is As.\"),\n",
        "    (\"Can you tell me about famous anthropologists?\", \"Sure! There are many famous anthropologists, including Margaret Mead, Franz Boas, and Claude L√©vi-Strauss.\"),\n",
        "    (\"What's the chemical symbol for boron?\", \"The chemical symbol for boron is B.\"),\n",
        "    (\"Can you tell me about famous economists?\", \"Sure! There are many famous economists, including Adam Smith, John Maynard Keynes, and Milton Friedman.\"),\n",
        "    # Add more conversation pairs\n",
        "]\n",
        "\n",
        "# Extract text from tuples\n",
        "questions, answers = zip(*conversations)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(list(questions) + list(answers))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Add <start> and <end> tokens\n",
        "tokenizer.word_index['<start>'] = vocab_size\n",
        "tokenizer.word_index['<end>'] = vocab_size + 1\n",
        "vocab_size += 2\n",
        "\n",
        "# Convert text to sequences\n",
        "X_sequences = tokenizer.texts_to_sequences(questions)\n",
        "y_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Padding sequences\n",
        "max_length = max(max(len(seq) for seq in X_sequences), max(len(seq) for seq in y_sequences))\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_length, padding='post')\n",
        "y_padded = pad_sequences(y_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define model architecture\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = Input(shape=(max_length,))\n",
        "encoder_inputs_reshaped = tf.expand_dims(encoder_inputs, axis=-1)\n",
        "encoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs_reshaped)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(1,))\n",
        "decoder_inputs_reshaped = tf.expand_dims(decoder_inputs, axis=-1)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_reshaped, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Training the model\n",
        "model.fit([X_padded, y_padded], y_padded, batch_size=64, epochs=100)\n",
        "\n",
        "# Inference\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs_reshaped, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "def respond(input_text):\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
        "    input_seq = np.expand_dims(input_seq, axis=-1)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
        "\n",
        "    stop_condition = False\n",
        "    response = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 0:  # Check for out-of-vocabulary token\n",
        "            sampled_word = '<unk>'  # Handle unknown token\n",
        "        else:\n",
        "            sampled_word = tokenizer.index_word.get(sampled_token_index, '<unk>')  # Get word or handle unknown token\n",
        "\n",
        "        if sampled_word == '<end>' or len(response.split()) > max_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            response += sampled_word + ' '\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "user_input = \"What's your name?\"\n",
        "response = respond(user_input)\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQQ8Es38Tctx",
        "outputId": "df252558-3e97-4967-9805-82cfd278ddc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - 4s 229ms/step - loss: 6.5582\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - 1s 217ms/step - loss: 5.5748\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 4.0006\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - 1s 362ms/step - loss: 2.9205\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 2.4697\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 2.3061\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - 1s 228ms/step - loss: 2.2078\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 2.1441\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 2.0920\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 2.0489\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 2.0129\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - 1s 268ms/step - loss: 1.9760\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 1.9406\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 1.9082\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - 1s 231ms/step - loss: 1.8774\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - 1s 227ms/step - loss: 1.8476\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 1.8195\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 1.7919\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 1.7640\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 1.7374\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - 1s 335ms/step - loss: 1.7111\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - 1s 226ms/step - loss: 1.6861\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - 1s 232ms/step - loss: 1.6609\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - 1s 231ms/step - loss: 1.6367\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 1.6131\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 1.5905\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - 1s 233ms/step - loss: 1.5683\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 1.5460\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 1.5251\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - 1s 232ms/step - loss: 1.5040\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 1.4839\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - 1s 221ms/step - loss: 1.4643\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - 1s 233ms/step - loss: 1.4451\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - 1s 221ms/step - loss: 1.4256\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - 1s 251ms/step - loss: 1.4070\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 1.3889\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - 1s 365ms/step - loss: 1.3712\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - 1s 227ms/step - loss: 1.3531\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 1.3364\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - 1s 228ms/step - loss: 1.3199\n",
            "Epoch 41/100\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 1.3042\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - 1s 224ms/step - loss: 1.2881\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - 1s 431ms/step - loss: 1.2727\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - 1s 226ms/step - loss: 1.2575\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - 1s 227ms/step - loss: 1.2419\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - 1s 228ms/step - loss: 1.2276\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - 1s 228ms/step - loss: 1.2126\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - 1s 243ms/step - loss: 1.1993\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - 1s 226ms/step - loss: 1.1851\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 1.1715\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.1578\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 1.1455\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 1.1326\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 1.1202\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - 1s 287ms/step - loss: 1.1078\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 1.0956\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - 1s 227ms/step - loss: 1.0843\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - 1s 218ms/step - loss: 1.0719\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 1.0610\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - 1s 462ms/step - loss: 1.0493\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 1.0380\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 1.0271\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 1.0165\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 1.0059\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 0.9956\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 0.9850\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 0.9754\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 0.9643\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 0.9547\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 0.9444\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - 1s 228ms/step - loss: 0.9353\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - 1s 226ms/step - loss: 0.9258\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - 1s 226ms/step - loss: 0.9161\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - 1s 231ms/step - loss: 0.9070\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - 1s 226ms/step - loss: 0.8976\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - 1s 231ms/step - loss: 0.8888\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - 1s 231ms/step - loss: 0.8795\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - 1s 229ms/step - loss: 0.8707\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 0.8615\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - 1s 232ms/step - loss: 0.8523\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.8437\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - 1s 356ms/step - loss: 0.8353\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - 1s 236ms/step - loss: 0.8269\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 0.8190\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - 1s 227ms/step - loss: 0.8101\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 0.8018\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - 1s 235ms/step - loss: 0.7934\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 0.7851\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 0.7769\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.7690\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - 1s 230ms/step - loss: 0.7608\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 0.7535\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - 1s 235ms/step - loss: 0.7453\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - 1s 236ms/step - loss: 0.7380\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - 1s 233ms/step - loss: 0.7307\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - 1s 231ms/step - loss: 0.7240\n",
            "Epoch 97/100\n",
            "3/3 [==============================] - 1s 365ms/step - loss: 0.7171\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.7094\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - 1s 304ms/step - loss: 0.7019\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - 1s 233ms/step - loss: 0.6950\n",
            "1/1 [==============================] - 0s 400ms/step\n",
            "1/1 [==============================] - 0s 420ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Response: india subjective, you. you. you. you. they they everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! everything! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample conversation data\n",
        "conversations = [\n",
        "    (\"Hello.\", \"Hi there!\"),\n",
        "    (\"How are you?\", \"I'm good, thank you.\"),\n",
        "    (\"What's your name?\", \"I am an AI chatbot.\"),\n",
        "    # Add more conversation pairs\n",
        "]\n",
        "\n",
        "# Extract text from tuples\n",
        "questions, answers = zip(*conversations)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(list(questions) + list(answers))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Add <start> and <end> tokens\n",
        "tokenizer.word_index['<start>'] = vocab_size\n",
        "tokenizer.word_index['<end>'] = vocab_size + 1\n",
        "vocab_size += 2\n",
        "\n",
        "# Convert text to sequences\n",
        "X_sequences = tokenizer.texts_to_sequences(questions)\n",
        "y_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Padding sequences\n",
        "max_length = max(max(len(seq) for seq in X_sequences), max(len(seq) for seq in y_sequences))\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_length, padding='post')\n",
        "y_padded = pad_sequences(y_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define model architecture\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = Input(shape=(max_length,))\n",
        "encoder_inputs_reshaped = tf.expand_dims(encoder_inputs, axis=-1)\n",
        "encoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs_reshaped)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(1,))\n",
        "decoder_inputs_reshaped = tf.expand_dims(decoder_inputs, axis=-1)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_reshaped, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Training the model\n",
        "model.fit([X_padded, y_padded], y_padded, batch_size=64, epochs=100)\n",
        "\n",
        "# Inference\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs_reshaped, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "def respond(input_text):\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
        "    input_seq = np.expand_dims(input_seq, axis=-1)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
        "\n",
        "    stop_condition = False\n",
        "    response = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 0:  # Check for out-of-vocabulary token\n",
        "            sampled_word = '<unk>'  # Handle unknown token\n",
        "        else:\n",
        "            sampled_word = tokenizer.index_word.get(sampled_token_index, '<unk>')  # Get word or handle unknown token\n",
        "\n",
        "        if sampled_word == '<end>' or len(response.split()) > max_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            response += sampled_word + ' '\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "user_input = \"Hy how are you\"\n",
        "response = respond(user_input)\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5XlfQu1XlJD",
        "outputId": "2217976f-3189-43a5-cced-a6b93ac7b410"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 5s 5s/step - loss: 3.0765\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.8860\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.7104\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.5481\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.3986\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.2612\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 2.1335\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.0114\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.8915\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.7748\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.6678\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.5779\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.5037\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.4332\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.3580\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.2829\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2174\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.1596\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0996\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.0357\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.9765\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9235\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8666\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.8088\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7584\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.7076\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6537\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6050\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.5565\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.5085\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4660\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4223\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3818\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3460\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3111\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2808\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2524\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2256\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2016\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1781\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1569\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.1375\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1194\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1040\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0907\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0793\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0701\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0621\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0551\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0490\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0435\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0387\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0349\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0316\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0288\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0265\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0244\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0226\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0210\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0195\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0182\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0171\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0160\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0151\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0143\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0135\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0129\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0123\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0118\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0113\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0108\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0104\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0100\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0097\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0093\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0090\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0087\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0084\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0082\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0079\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0077\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0075\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0073\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0071\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0069\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0068\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0066\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0064\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0063\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0062\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0060\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0059\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0058\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0057\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0056\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0054\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0053\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0052\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0052\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0051\n",
            "1/1 [==============================] - 0s 411ms/step\n",
            "1/1 [==============================] - 0s 414ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Response: i'm thank you. <unk> <unk> <unk> \n"
          ]
        }
      ]
    }
  ]
}